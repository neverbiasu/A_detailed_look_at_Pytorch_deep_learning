{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例4 1】  Numpy构建神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30616220.806829024\n",
      "100 350.82846029385155\n",
      "200 1.16945339805917\n",
      "300 0.006360159087620377\n",
      "400 4.4552531957579504e-05\n"
     ]
    }
   ],
   "source": [
    "# N是批量大小; D_in是输入维度;\n",
    "# H是隐藏的维度; D_out是输出维度。\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 创建随机输入和输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传递：计算预测值y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 计算和打印损失loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 反向传播，计算w1和w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度是什么？（gradient）是一种关于多元导数个概括\n",
    "#### 我们需要什么梯度？ 损失函数相对于权重的梯度\n",
    "#### 这个计算过程需要链式规则一步一步计算，从右往左所以叫反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例4 2】  Pytorch建立两层神经网络，实现前向传播和反向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39350560.0\n",
      "100 403.9136657714844\n",
      "200 1.794999599456787\n",
      "300 0.013088712468743324\n",
      "400 0.0003061269235331565\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device（“cuda：0”）＃取消注释以在GPU上运行\n",
    "\n",
    "# N是批量大小; D_in是输入维度;\n",
    "# H是隐藏的维度; D_out是输出维度。\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "#创建随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传递：计算预测y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 计算和打印损失\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop计算w1和w2相对于损耗的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例4 3】  Pytorch优化模块optim优化器调用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 644.3152465820312\n",
      "100 41.55706024169922\n",
      "200 0.4714313745498657\n",
      "300 0.003095320425927639\n",
      "400 1.3526137990993448e-05\n"
     ]
    }
   ],
   "source": [
    "# N是批大小；D是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 使用nn包定义模型和损失函数\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 使用optim包定义优化器（Optimizer）。Optimizer将会为我们更新模型的权重。\n",
    "# 这里我们使用Adam优化方法；optim包还包含了许多别的优化算法。\n",
    "# Adam构造函数的第一个参数告诉优化器应该更新哪些张量。\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "\n",
    "    # 前向传播：通过像模型输入x计算预测的y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并打印loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 在反向传播之前，使用optimizer将它要更新的所有张量的梯度清零(这些张量是模型可学习的权重)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 反向传播：根据模型的参数计算loss的梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 调用Optimizer的step函数使它所有参数更新\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例4 4】  Pytorch自定义torch.nn.Module的子类构建两层网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 648.269287109375\n",
      "100 2.4362049102783203\n",
      "200 0.04835866391658783\n",
      "300 0.002740337513387203\n",
      "400 0.00021167816885281354\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们实例化了两个nn.Linear模块，并将它们作为成员变量。\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        在前向传播的函数中，我们接收一个输入的张量，也必须返回一个输出张量。\n",
    "        我们可以使用构造函数中定义的模块以及张量上的任意的（可微分的）操作。\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# N是批大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生输入和输出的随机张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 通过实例化上面定义的类来构建我们的模型。\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 构造损失函数和优化器。\n",
    "# SGD构造函数中对model.parameters()的调用，\n",
    "# 将包含模型的一部分，即两个nn.Linear模块的可学习参数。\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 前向传播：通过向模型传递x计算预测值y\n",
    "    y_pred = model(x)\n",
    "\n",
    "    #计算并输出loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 清零梯度，反向传播，更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例4 5】  Pytorch网络权重共享。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 676.591064453125\n",
      "100 37.59136199951172\n",
      "200 2.0575761795043945\n",
      "300 2.403402328491211\n",
      "400 0.3576049506664276\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        在构造函数中，我们构造了三个nn.Linear实例，它们将在前向传播时被使用。\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        对于模型的前向传播，我们随机选择0、1、2、3，\n",
    "        并重用了多次计算隐藏层的middle_linear模块。\n",
    "        由于每个前向传播构建一个动态计算图，\n",
    "        我们可以在定义模型的前向传播时使用常规Python控制流运算符，如循环或条件语句。\n",
    "        在这里，我们还看到，在定义计算图形时多次重用同一个模块是完全安全的。\n",
    "        这是Lua Torch的一大改进，因为Lua Torch中每个模块只能使用一次。\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N是批大小；D是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生输入和输出随机张量\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 实例化上面定义的类来构造我们的模型\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 构造损失函数（loss function）和优化器（Optimizer）。\n",
    "# 用平凡的随机梯度下降训练这个奇怪的模型是困难的，所以我们使用了momentum方法。\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "\n",
    "    # 前向传播：通过向模型传入x计算预测的y。\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算并打印损失\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 清零梯度，反向传播，更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例4 6】  Pytorch搭建一个全连接神经网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.45it/s, train_loss=0.444, val_loss=0.466]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.13it/s, train_loss=0.383, val_loss=0.404] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 64.69it/s, train_loss=0.346, val_loss=0.375] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.36it/s, train_loss=0.32, val_loss=0.354]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.21it/s, train_loss=0.298, val_loss=0.338]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.67it/s, train_loss=0.279, val_loss=0.323]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.88it/s, train_loss=0.262, val_loss=0.31]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.68it/s, train_loss=0.246, val_loss=0.297]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.73it/s, train_loss=0.231, val_loss=0.286]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.27it/s, train_loss=0.216, val_loss=0.274]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.03it/s, train_loss=0.201, val_loss=0.263] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.20it/s, train_loss=0.187, val_loss=0.253]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.46it/s, train_loss=0.173, val_loss=0.242] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.90it/s, train_loss=0.159, val_loss=0.232]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.02it/s, train_loss=0.146, val_loss=0.223]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.57it/s, train_loss=0.133, val_loss=0.213] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.37it/s, train_loss=0.12, val_loss=0.205]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.15it/s, train_loss=0.108, val_loss=0.196]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.66it/s, train_loss=0.097, val_loss=0.188]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.12it/s, train_loss=0.0865, val_loss=0.181]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.68it/s, train_loss=0.0767, val_loss=0.175]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 64.36it/s, train_loss=0.0677, val_loss=0.169]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.74it/s, train_loss=0.0595, val_loss=0.164] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.14it/s, train_loss=0.0521, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.74it/s, train_loss=0.0455, val_loss=0.156]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 62.82it/s, train_loss=0.0398, val_loss=0.153] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.02it/s, train_loss=0.0347, val_loss=0.15]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.29it/s, train_loss=0.0304, val_loss=0.148]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.16it/s, train_loss=0.0267, val_loss=0.147]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.78it/s, train_loss=0.0235, val_loss=0.146] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.02it/s, train_loss=0.021, val_loss=0.145]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.60it/s, train_loss=0.0187, val_loss=0.145]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.92it/s, train_loss=0.017, val_loss=0.145]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.03it/s, train_loss=0.0155, val_loss=0.145]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 60.00it/s, train_loss=0.0142, val_loss=0.145] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.82it/s, train_loss=0.0131, val_loss=0.146]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.93it/s, train_loss=0.0123, val_loss=0.146]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.27it/s, train_loss=0.0115, val_loss=0.147]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.07it/s, train_loss=0.0109, val_loss=0.147]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.06it/s, train_loss=0.0103, val_loss=0.147]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.02it/s, train_loss=0.00987, val_loss=0.148]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.74it/s, train_loss=0.00945, val_loss=0.148]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.91it/s, train_loss=0.00906, val_loss=0.149]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 60.75it/s, train_loss=0.00876, val_loss=0.15]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.39it/s, train_loss=0.00846, val_loss=0.15]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.61it/s, train_loss=0.00823, val_loss=0.151]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.91it/s, train_loss=0.00802, val_loss=0.152]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.43it/s, train_loss=0.00789, val_loss=0.152]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.46it/s, train_loss=0.00771, val_loss=0.153]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.34it/s, train_loss=0.0076, val_loss=0.154]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.03it/s, train_loss=0.00752, val_loss=0.155]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.14it/s, train_loss=0.00738, val_loss=0.156]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.43it/s, train_loss=0.00735, val_loss=0.157]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 59.04it/s, train_loss=0.00733, val_loss=0.158]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.64it/s, train_loss=0.00724, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.43it/s, train_loss=0.00733, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.91it/s, train_loss=0.00746, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.61it/s, train_loss=0.00772, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.87it/s, train_loss=0.00869, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.62it/s, train_loss=0.0114, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.79it/s, train_loss=0.0175, val_loss=0.174]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.30it/s, train_loss=0.0283, val_loss=0.19]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.03it/s, train_loss=0.04, val_loss=0.212]   \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.40it/s, train_loss=0.0368, val_loss=0.215]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.72it/s, train_loss=0.0264, val_loss=0.186]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.63it/s, train_loss=0.0532, val_loss=0.186]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.61it/s, train_loss=0.0601, val_loss=0.197]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.30it/s, train_loss=0.023, val_loss=0.165]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.53it/s, train_loss=0.021, val_loss=0.157]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.07it/s, train_loss=0.0243, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.72it/s, train_loss=0.0198, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.31it/s, train_loss=0.0128, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.48it/s, train_loss=0.0087, val_loss=0.157] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.44it/s, train_loss=0.00657, val_loss=0.154]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.86it/s, train_loss=0.00556, val_loss=0.152]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.60it/s, train_loss=0.0052, val_loss=0.15] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.85it/s, train_loss=0.00512, val_loss=0.15]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.23it/s, train_loss=0.00512, val_loss=0.15]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.28it/s, train_loss=0.00505, val_loss=0.15]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.37it/s, train_loss=0.00499, val_loss=0.15]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.22it/s, train_loss=0.00498, val_loss=0.151]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.06it/s, train_loss=0.00502, val_loss=0.152]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.03it/s, train_loss=0.00505, val_loss=0.152]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.25it/s, train_loss=0.00502, val_loss=0.153]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.92it/s, train_loss=0.00487, val_loss=0.154]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.97it/s, train_loss=0.00459, val_loss=0.155]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.18it/s, train_loss=0.00418, val_loss=0.155]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.90it/s, train_loss=0.00376, val_loss=0.155]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.98it/s, train_loss=0.00339, val_loss=0.155]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.59it/s, train_loss=0.0032, val_loss=0.154]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.51it/s, train_loss=0.00326, val_loss=0.154]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.03it/s, train_loss=0.00354, val_loss=0.155]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 64.41it/s, train_loss=0.00406, val_loss=0.155]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.39it/s, train_loss=0.00471, val_loss=0.156]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.11it/s, train_loss=0.00535, val_loss=0.158]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.99it/s, train_loss=0.0059, val_loss=0.159] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.39it/s, train_loss=0.0062, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 60.31it/s, train_loss=0.00628, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.43it/s, train_loss=0.00631, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.66it/s, train_loss=0.00632, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 93.11it/s, train_loss=0.0067, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.44it/s, train_loss=0.00739, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.49it/s, train_loss=0.00857, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.93it/s, train_loss=0.00991, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.93it/s, train_loss=0.0111, val_loss=0.17] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.11it/s, train_loss=0.0115, val_loss=0.171] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.44it/s, train_loss=0.0115, val_loss=0.172]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.48it/s, train_loss=0.0124, val_loss=0.172]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.60it/s, train_loss=0.0143, val_loss=0.173]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.74it/s, train_loss=0.016, val_loss=0.174]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.51it/s, train_loss=0.0149, val_loss=0.172]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.26it/s, train_loss=0.0123, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.35it/s, train_loss=0.0122, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.83it/s, train_loss=0.0138, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.21it/s, train_loss=0.0138, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.46it/s, train_loss=0.0105, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.69it/s, train_loss=0.00651, val_loss=0.158]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.05it/s, train_loss=0.00448, val_loss=0.156]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.15it/s, train_loss=0.00427, val_loss=0.156]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.50it/s, train_loss=0.00468, val_loss=0.158]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.82it/s, train_loss=0.00499, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.05it/s, train_loss=0.00495, val_loss=0.16]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.92it/s, train_loss=0.00486, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.53it/s, train_loss=0.00459, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.08it/s, train_loss=0.00405, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.80it/s, train_loss=0.0034, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.09it/s, train_loss=0.00286, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.10it/s, train_loss=0.0027, val_loss=0.159]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.69it/s, train_loss=0.00296, val_loss=0.159] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.46it/s, train_loss=0.00346, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.15it/s, train_loss=0.00385, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.31it/s, train_loss=0.00401, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 60.91it/s, train_loss=0.00384, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.65it/s, train_loss=0.00358, val_loss=0.159] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.37it/s, train_loss=0.00356, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.99it/s, train_loss=0.00399, val_loss=0.16] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.51it/s, train_loss=0.00483, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.81it/s, train_loss=0.00586, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.55it/s, train_loss=0.00656, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.12it/s, train_loss=0.00661, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.68it/s, train_loss=0.00569, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 93.21it/s, train_loss=0.00436, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 64.51it/s, train_loss=0.00331, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.03it/s, train_loss=0.00365, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.86it/s, train_loss=0.00611, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.74it/s, train_loss=0.00993, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 93.15it/s, train_loss=0.013, val_loss=0.172]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.28it/s, train_loss=0.013, val_loss=0.177] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.14it/s, train_loss=0.00946, val_loss=0.177]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.46it/s, train_loss=0.00511, val_loss=0.171]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.37it/s, train_loss=0.00428, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.76it/s, train_loss=0.00811, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 66.41it/s, train_loss=0.0123, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.40it/s, train_loss=0.0121, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.03it/s, train_loss=0.00844, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.76it/s, train_loss=0.00521, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.12it/s, train_loss=0.00492, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 60.38it/s, train_loss=0.00574, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.05it/s, train_loss=0.00568, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.72it/s, train_loss=0.00402, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.15it/s, train_loss=0.00242, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.66it/s, train_loss=0.00209, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 59.11it/s, train_loss=0.00288, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.76it/s, train_loss=0.00367, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.68it/s, train_loss=0.00375, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.93it/s, train_loss=0.00316, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 63.23it/s, train_loss=0.00262, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.83it/s, train_loss=0.00245, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.69it/s, train_loss=0.00275, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.59it/s, train_loss=0.00308, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.28it/s, train_loss=0.00321, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 58.43it/s, train_loss=0.00299, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.82it/s, train_loss=0.00262, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.31it/s, train_loss=0.00247, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.31it/s, train_loss=0.00277, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.82it/s, train_loss=0.00346, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.53it/s, train_loss=0.0046, val_loss=0.163]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.98it/s, train_loss=0.00575, val_loss=0.165] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.95it/s, train_loss=0.00666, val_loss=0.168]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.39it/s, train_loss=0.00679, val_loss=0.17]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.40it/s, train_loss=0.00607, val_loss=0.171]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.73it/s, train_loss=0.00471, val_loss=0.17] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.79it/s, train_loss=0.00367, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.47it/s, train_loss=0.0045, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.37it/s, train_loss=0.00832, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.73it/s, train_loss=0.0133, val_loss=0.174]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.07it/s, train_loss=0.0159, val_loss=0.18] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.53it/s, train_loss=0.0129, val_loss=0.181]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.12it/s, train_loss=0.00803, val_loss=0.174]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.82it/s, train_loss=0.00608, val_loss=0.168]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.04it/s, train_loss=0.00759, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.49it/s, train_loss=0.00932, val_loss=0.165] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.39it/s, train_loss=0.00825, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.53it/s, train_loss=0.0059, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.66it/s, train_loss=0.00568, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.23it/s, train_loss=0.00632, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.16it/s, train_loss=0.0064, val_loss=0.164]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.87it/s, train_loss=0.00536, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.91it/s, train_loss=0.00405, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.00it/s, train_loss=0.00326, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.49it/s, train_loss=0.00311, val_loss=0.164] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.79it/s, train_loss=0.00329, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.28it/s, train_loss=0.00363, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.61it/s, train_loss=0.00417, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.15it/s, train_loss=0.00442, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.41it/s, train_loss=0.00416, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.85it/s, train_loss=0.00332, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.16it/s, train_loss=0.00241, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.77it/s, train_loss=0.00213, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.99it/s, train_loss=0.0029, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.53it/s, train_loss=0.00444, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.01it/s, train_loss=0.00609, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.11it/s, train_loss=0.00657, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.65it/s, train_loss=0.0057, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.52it/s, train_loss=0.00378, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.62it/s, train_loss=0.00214, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.81it/s, train_loss=0.00201, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.15it/s, train_loss=0.0038, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.01it/s, train_loss=0.00643, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.11it/s, train_loss=0.0082, val_loss=0.171]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.45it/s, train_loss=0.00783, val_loss=0.173]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.97it/s, train_loss=0.00571, val_loss=0.172] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.11it/s, train_loss=0.00351, val_loss=0.169]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.16it/s, train_loss=0.00288, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.33it/s, train_loss=0.00383, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.24it/s, train_loss=0.00518, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.17it/s, train_loss=0.00566, val_loss=0.165] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.30it/s, train_loss=0.00515, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.57it/s, train_loss=0.00455, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.07it/s, train_loss=0.00454, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.25it/s, train_loss=0.0047, val_loss=0.165] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 66.77it/s, train_loss=0.00433, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.72it/s, train_loss=0.00328, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.90it/s, train_loss=0.00226, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.76it/s, train_loss=0.00226, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.23it/s, train_loss=0.00323, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 56.12it/s, train_loss=0.00453, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.57it/s, train_loss=0.0052, val_loss=0.167] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.92it/s, train_loss=0.00489, val_loss=0.169]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.75it/s, train_loss=0.00408, val_loss=0.169]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.64it/s, train_loss=0.00356, val_loss=0.168]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.99it/s, train_loss=0.00346, val_loss=0.167] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.48it/s, train_loss=0.00372, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.36it/s, train_loss=0.00379, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.00it/s, train_loss=0.00405, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.38it/s, train_loss=0.0048, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.38it/s, train_loss=0.00604, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.86it/s, train_loss=0.00691, val_loss=0.169]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.01it/s, train_loss=0.00674, val_loss=0.17]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.45it/s, train_loss=0.00553, val_loss=0.17] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.76it/s, train_loss=0.00477, val_loss=0.17]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.54it/s, train_loss=0.00498, val_loss=0.169] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.90it/s, train_loss=0.00563, val_loss=0.17]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.89it/s, train_loss=0.00575, val_loss=0.169]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.03it/s, train_loss=0.00573, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.03it/s, train_loss=0.00631, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 64.83it/s, train_loss=0.00741, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.89it/s, train_loss=0.00744, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.23it/s, train_loss=0.0061, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.93it/s, train_loss=0.00456, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.30it/s, train_loss=0.0039, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.20it/s, train_loss=0.004, val_loss=0.166]   \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.95it/s, train_loss=0.00408, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.44it/s, train_loss=0.00405, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.21it/s, train_loss=0.00457, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.67it/s, train_loss=0.00519, val_loss=0.165] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.29it/s, train_loss=0.00506, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.76it/s, train_loss=0.00413, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.73it/s, train_loss=0.00263, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.18it/s, train_loss=0.00213, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.72it/s, train_loss=0.00303, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.41it/s, train_loss=0.00459, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.49it/s, train_loss=0.0054, val_loss=0.166]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.00it/s, train_loss=0.00476, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.75it/s, train_loss=0.00347, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.52it/s, train_loss=0.0025, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.38it/s, train_loss=0.00224, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.62it/s, train_loss=0.00258, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.27it/s, train_loss=0.00307, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 53.75it/s, train_loss=0.0035, val_loss=0.165] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.33it/s, train_loss=0.00365, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.62it/s, train_loss=0.00352, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.15it/s, train_loss=0.00307, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.22it/s, train_loss=0.00256, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 57.82it/s, train_loss=0.00218, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.57it/s, train_loss=0.00257, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.37it/s, train_loss=0.00342, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.40it/s, train_loss=0.00426, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 64.21it/s, train_loss=0.00431, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.74it/s, train_loss=0.00346, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.23it/s, train_loss=0.00235, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.91it/s, train_loss=0.00195, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.91it/s, train_loss=0.00295, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.02it/s, train_loss=0.00467, val_loss=0.169] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.60it/s, train_loss=0.00601, val_loss=0.171] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.94it/s, train_loss=0.00609, val_loss=0.171]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.82it/s, train_loss=0.00536, val_loss=0.17]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.35it/s, train_loss=0.00501, val_loss=0.168]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.04it/s, train_loss=0.00566, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.99it/s, train_loss=0.00629, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 59.65it/s, train_loss=0.00552, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.85it/s, train_loss=0.0042, val_loss=0.165] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.32it/s, train_loss=0.00455, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.85it/s, train_loss=0.0077, val_loss=0.167]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.99it/s, train_loss=0.0111, val_loss=0.171]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.54it/s, train_loss=0.0111, val_loss=0.174]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.43it/s, train_loss=0.00701, val_loss=0.171]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.93it/s, train_loss=0.00325, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 61.64it/s, train_loss=0.00314, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.75it/s, train_loss=0.0055, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.96it/s, train_loss=0.00718, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.03it/s, train_loss=0.00639, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.76it/s, train_loss=0.00443, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.43it/s, train_loss=0.00299, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.20it/s, train_loss=0.00222, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.66it/s, train_loss=0.00209, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.32it/s, train_loss=0.00208, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.74it/s, train_loss=0.00229, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.83it/s, train_loss=0.00241, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.77it/s, train_loss=0.00227, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.85it/s, train_loss=0.00192, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.76it/s, train_loss=0.00168, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.48it/s, train_loss=0.00163, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.14it/s, train_loss=0.0018, val_loss=0.164] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.37it/s, train_loss=0.00187, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.35it/s, train_loss=0.00172, val_loss=0.164] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.95it/s, train_loss=0.00139, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.49it/s, train_loss=0.0012, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.49it/s, train_loss=0.00149, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.98it/s, train_loss=0.00224, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.69it/s, train_loss=0.00303, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.80it/s, train_loss=0.00337, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.45it/s, train_loss=0.00303, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.01it/s, train_loss=0.00227, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.23it/s, train_loss=0.00183, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.09it/s, train_loss=0.00219, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.46it/s, train_loss=0.00345, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.47it/s, train_loss=0.00462, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.14it/s, train_loss=0.00507, val_loss=0.168]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.09it/s, train_loss=0.00485, val_loss=0.168]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.88it/s, train_loss=0.00491, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.21it/s, train_loss=0.00587, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.59it/s, train_loss=0.00733, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 62.61it/s, train_loss=0.00712, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.27it/s, train_loss=0.0049, val_loss=0.166]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.04it/s, train_loss=0.00378, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.58it/s, train_loss=0.00623, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.60it/s, train_loss=0.00984, val_loss=0.17]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 66.05it/s, train_loss=0.00948, val_loss=0.172]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.07it/s, train_loss=0.00557, val_loss=0.168]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.05it/s, train_loss=0.00318, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.14it/s, train_loss=0.00451, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 63.71it/s, train_loss=0.0054, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.40it/s, train_loss=0.00384, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.61it/s, train_loss=0.00238, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.50it/s, train_loss=0.00274, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.14it/s, train_loss=0.00344, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.72it/s, train_loss=0.00303, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.14it/s, train_loss=0.00187, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.93it/s, train_loss=0.00113, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.56it/s, train_loss=0.00129, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 92.94it/s, train_loss=0.00189, val_loss=0.16] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.38it/s, train_loss=0.00231, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.39it/s, train_loss=0.00233, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.78it/s, train_loss=0.00194, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.15it/s, train_loss=0.00142, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.50it/s, train_loss=0.00105, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 60.19it/s, train_loss=0.00112, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.80it/s, train_loss=0.00174, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.42it/s, train_loss=0.00272, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.98it/s, train_loss=0.00351, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.04it/s, train_loss=0.00367, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.85it/s, train_loss=0.00302, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.74it/s, train_loss=0.00219, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.31it/s, train_loss=0.00196, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.99it/s, train_loss=0.00289, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 61.72it/s, train_loss=0.00431, val_loss=0.168]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.64it/s, train_loss=0.00547, val_loss=0.17] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.45it/s, train_loss=0.00497, val_loss=0.17]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.89it/s, train_loss=0.00387, val_loss=0.167] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.78it/s, train_loss=0.0038, val_loss=0.164] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.41it/s, train_loss=0.0057, val_loss=0.164] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.12it/s, train_loss=0.00781, val_loss=0.165] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.89it/s, train_loss=0.00754, val_loss=0.167]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.44it/s, train_loss=0.00581, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.53it/s, train_loss=0.00505, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 61.50it/s, train_loss=0.00598, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.46it/s, train_loss=0.00606, val_loss=0.166] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.80it/s, train_loss=0.0047, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.15it/s, train_loss=0.0037, val_loss=0.163]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.40it/s, train_loss=0.00455, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.00it/s, train_loss=0.00564, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.99it/s, train_loss=0.00518, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.71it/s, train_loss=0.0039, val_loss=0.167] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.91it/s, train_loss=0.00305, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.90it/s, train_loss=0.00302, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.46it/s, train_loss=0.00301, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.03it/s, train_loss=0.00272, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.18it/s, train_loss=0.00237, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 59.86it/s, train_loss=0.00258, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.08it/s, train_loss=0.00309, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.92it/s, train_loss=0.00338, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.04it/s, train_loss=0.00315, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.05it/s, train_loss=0.00255, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.86it/s, train_loss=0.00203, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.29it/s, train_loss=0.00215, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 65.43it/s, train_loss=0.00292, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.28it/s, train_loss=0.00374, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.74it/s, train_loss=0.00373, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 57.74it/s, train_loss=0.00282, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.92it/s, train_loss=0.00186, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.00it/s, train_loss=0.00191, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.07it/s, train_loss=0.0034, val_loss=0.164]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 63.65it/s, train_loss=0.00505, val_loss=0.167] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.60it/s, train_loss=0.00561, val_loss=0.169]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.28it/s, train_loss=0.00457, val_loss=0.169]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.65it/s, train_loss=0.00299, val_loss=0.166]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 73.30it/s, train_loss=0.00224, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.10it/s, train_loss=0.00251, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.15it/s, train_loss=0.00327, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.41it/s, train_loss=0.00404, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.23it/s, train_loss=0.00461, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.16it/s, train_loss=0.00472, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 63.00it/s, train_loss=0.00408, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.33it/s, train_loss=0.00317, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.30it/s, train_loss=0.00275, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.86it/s, train_loss=0.00316, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.44it/s, train_loss=0.004, val_loss=0.163]   \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.94it/s, train_loss=0.00423, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 81.55it/s, train_loss=0.00385, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.31it/s, train_loss=0.00336, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.95it/s, train_loss=0.00332, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.99it/s, train_loss=0.00349, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.51it/s, train_loss=0.00341, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.07it/s, train_loss=0.00311, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.72it/s, train_loss=0.00304, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.10it/s, train_loss=0.00313, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.59it/s, train_loss=0.00313, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.52it/s, train_loss=0.00285, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.39it/s, train_loss=0.00246, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.30it/s, train_loss=0.00252, val_loss=0.16] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.79it/s, train_loss=0.00315, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.49it/s, train_loss=0.00364, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 62.13it/s, train_loss=0.00351, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.27it/s, train_loss=0.00276, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.81it/s, train_loss=0.00211, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.33it/s, train_loss=0.00211, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 63.45it/s, train_loss=0.00267, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.40it/s, train_loss=0.00326, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.69it/s, train_loss=0.00331, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.77it/s, train_loss=0.00278, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.63it/s, train_loss=0.00234, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.64it/s, train_loss=0.00229, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.25it/s, train_loss=0.00266, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.92it/s, train_loss=0.00316, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.22it/s, train_loss=0.0034, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.17it/s, train_loss=0.00337, val_loss=0.165]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 63.88it/s, train_loss=0.00298, val_loss=0.164] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.90it/s, train_loss=0.00257, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 86.42it/s, train_loss=0.00266, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.91it/s, train_loss=0.00338, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.85it/s, train_loss=0.0042, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 78.38it/s, train_loss=0.0044, val_loss=0.162]  \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 82.94it/s, train_loss=0.00386, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 69.82it/s, train_loss=0.00346, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.75it/s, train_loss=0.00371, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.32it/s, train_loss=0.00439, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 62.04it/s, train_loss=0.00443, val_loss=0.164] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 83.33it/s, train_loss=0.00362, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.85it/s, train_loss=0.00316, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 89.16it/s, train_loss=0.00382, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.11it/s, train_loss=0.00484, val_loss=0.163]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.33it/s, train_loss=0.00479, val_loss=0.164]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 80.83it/s, train_loss=0.00362, val_loss=0.163] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 74.23it/s, train_loss=0.00288, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 90.22it/s, train_loss=0.0032, val_loss=0.161] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 88.77it/s, train_loss=0.00375, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.41it/s, train_loss=0.00337, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 67.37it/s, train_loss=0.00273, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 91.61it/s, train_loss=0.00293, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 72.15it/s, train_loss=0.00344, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 87.68it/s, train_loss=0.00319, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 77.21it/s, train_loss=0.00211, val_loss=0.16] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.31it/s, train_loss=0.00146, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.04it/s, train_loss=0.00189, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.54it/s, train_loss=0.0027, val_loss=0.16] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.46it/s, train_loss=0.00269, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 62.50it/s, train_loss=0.00193, val_loss=0.16]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.25it/s, train_loss=0.00118, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.14it/s, train_loss=0.00134, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 62.95it/s, train_loss=0.00201, val_loss=0.159]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 71.33it/s, train_loss=0.00249, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 76.71it/s, train_loss=0.00222, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 70.83it/s, train_loss=0.0016, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 85.77it/s, train_loss=0.00123, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 79.76it/s, train_loss=0.00145, val_loss=0.162]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 61.14it/s, train_loss=0.00187, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 75.30it/s, train_loss=0.00217, val_loss=0.162] \n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 84.08it/s, train_loss=0.00213, val_loss=0.161]\n",
      "Training...: 100%|██████████| 16/16 [00:00<00:00, 68.47it/s, train_loss=0.00201, val_loss=0.16]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(10)# 固定每次初始化模型的权重\n",
    "training_step = 500# 迭代次数\n",
    "batch_size = 512# 每个批次的大小\n",
    "n_features = 32# 特征数目\n",
    "M = 10000# 生成的数据数目\n",
    "# 生成数据\n",
    "data = np.random.randn(M,n_features)# 随机生成服从高斯分布的数据\n",
    "target = np.random.rand(M)\n",
    "\n",
    "# 特征归一化\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(data)\n",
    "data = min_max_scaler.transform(data)\n",
    "\n",
    "# 对训练集进行切割，然后进行训练\n",
    "x_train,x_val,y_train,y_val = train_test_split(data,target,test_size=0.2,shuffle=False)\n",
    "\n",
    "# 定义网络结构\n",
    "class Net(torch.nn.Module):  # 继承 torch 的 Module\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "        self.l1 = nn.Linear(n_features,500)#特征输入\n",
    "        self.l2 = nn.ReLU()# 激活函数\n",
    "        self.l3 = nn.BatchNorm1d(500)# 批标准化\n",
    "        self.l4 = nn.Linear(500,250)\n",
    "        self.l5 = nn.ReLU()\n",
    "        self.l6 = nn.BatchNorm1d(250)\n",
    "        self.l7 = nn.Linear(250,1)\n",
    "        #self.l8 = nn.Sigmoid()\n",
    "    def forward(self, inputs):   #  这同时也是 Module 中的 forward 功能\n",
    "        # 正向传播输入值, 神经网络分析出输出值\n",
    "        out = torch.from_numpy(inputs).to(torch.float32)# 将输入的numpy格式转换成tensor\n",
    "        out = self.l1(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.l6(out)\n",
    "        out = self.l7(out)\n",
    "        #out = self.l8(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "model = Net(n_features=n_features)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # 传入 net 的所有参数, 学习率\n",
    "# 定义目标损失函数\n",
    "loss_func = torch.nn.MSELoss() # 这里采用均方差函数\n",
    "\n",
    "# 开始迭代\n",
    "for step in range(training_step):\n",
    "    M_train = len(x_train)\n",
    "    with tqdm(np.arange(0,M_train,batch_size), desc='Training...') as tbar:\n",
    "        for index in tbar:\n",
    "            L = index\n",
    "            R = min(M_train,index+batch_size)\n",
    "            #-----------------训练内容------------------\n",
    "            train_pre = model(x_train[L:R,:])     # 喂给 model训练数据 x, 输出预测值\n",
    "            train_loss = loss_func(train_pre, torch.from_numpy(y_train[L:R].reshape(R-L,1)).to(torch.float32))\n",
    "            val_pre = model(x_val)\n",
    "            val_loss = loss_func(val_pre, torch.from_numpy(y_val.reshape(len(y_val),1)).to(torch.float32))\n",
    "            #-------------------------------------------\n",
    "            tbar.set_postfix(train_loss=float(train_loss.data),val_loss=float(val_loss.data))# 打印在进度条上\n",
    "            tbar.update()  # 默认参数n=1，每update一次，进度+n\n",
    "\n",
    "            #-----------------反向传播更新---------------\n",
    "            optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "            train_loss.backward()         # 以训练集的误差进行反向传播, 计算参数更新值\n",
    "            optimizer.step()        # 将参数更新值施加到 net 的 parameters 上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 【例4 7】  Pytorch搭建全连接神经网络，并打印查看网络结构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=32, out_features=500, bias=True)\n",
      "  (l2): ReLU()\n",
      "  (l3): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (l4): Linear(in_features=500, out_features=250, bias=True)\n",
      "  (l5): ReLU()\n",
      "  (l6): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (l7): Linear(in_features=250, out_features=1, bias=True)\n",
      "  (l8): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)#固定每次初始化模型的权重\n",
    "training_step = 500#迭代此时\n",
    "batch_size = 512#每个批次的大小\n",
    "n_features = 32#特征数目\n",
    "M = 10000#生成的数据数目\n",
    "#生成数据\n",
    "data = np.random.randn(M,n_features)#随机生成服从高斯分布的数据\n",
    "target = np.random.rand(M)\n",
    "\n",
    "#特征归一化\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(data)\n",
    "data = min_max_scaler.transform(data)\n",
    "\n",
    "# 对训练集进行切割，然后进行训练\n",
    "x_train,x_val,y_train,y_val = train_test_split(data,target,test_size=0.2,shuffle=False)\n",
    "\n",
    "#定义网络结构\n",
    "class Net(torch.nn.Module):  # 继承 torch 的 Module\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "        self.l1 = nn.Linear(n_features,500)#特征输入\n",
    "        self.l2 = nn.ReLU()#激活函数\n",
    "        self.l3 = nn.BatchNorm1d(500)#批标准化\n",
    "        self.l4 = nn.Linear(500,250)\n",
    "        self.l5 = nn.ReLU()\n",
    "        self.l6 = nn.BatchNorm1d(250)\n",
    "        self.l7 = nn.Linear(250,1)\n",
    "        self.l8 = nn.Sigmoid()\n",
    "    def forward(self, inputs):   # 这同时也是 Module 中的 forward 功能\n",
    "        # 正向传播输入值, 神经网络分析出输出值\n",
    "        out = torch.from_numpy(inputs).to(torch.float32)#将输入的numpy格式转换成tensor\n",
    "        out = self.l1(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.l6(out)\n",
    "        out = self.l7(out)\n",
    "        out = self.l8(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#定义模型\n",
    "model = Net(n_features=n_features)\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
