{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 实现过程\n",
    "#### 1. 输入$224\\times224\\times3$的图，经$64个3\\times3$的卷积核做两次卷积+ReLU，卷积后尺寸变为$244\\times244\\times64$\n",
    "#### 2. 进行最大池化（max pooling），池化单元尺寸为$2\\times2$（效果为尺寸减半），池化后变为$112\\times112\\times64$\n",
    "#### 3. 经128个$3\\times3$卷积核做两次卷积+ReLU，尺寸变为$112\\times112\\times128$\n",
    "#### 4. 做$2\\times2$的最大池化，尺寸变为$56\\times56\\times128$\n",
    "#### 5. 经过256个$3\\times3$的卷积核做3次卷积+ReLU，尺寸变为$56\\times56\\times256$\n",
    "#### 6. 做$2\\times2$的最大池化，尺寸变为$28\\times28\\times256$\n",
    "#### 7. 经过512个$3\\times3$的卷积核做3次卷积+ReLU，尺寸变为$28\\times28\\times512$\n",
    "#### 8. 做$2\\times2$的最大池化，尺寸变为$14\\times14\\times512$\n",
    "#### 9. 经过512个$3\\times3$的卷积核做3次卷积+ReLU，尺寸变为$14\\times14\\times512$ 假设步长为1且使用了“same”填充\n",
    "#### 10. 做$2\\times2$的最大池化，尺寸变为$7\\times7\\times512$\n",
    "#### 11. 与两层$1\\times1\\times4096$和一层$1\\times1\\times1000$进行全连接+ReLU（共三层）\n",
    "#### 12. 通过Softmax层输出1000个预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGNet的特点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 结构简洁\n",
    "##### 由5层卷积层、3层全连接层以及Softmax输出层构成，层与层之间使用最大池化分开，所有隐藏层的激活单元都采用ReLU函数\n",
    "#### 2. 小卷积核和多卷积子层\n",
    "##### 使用多个较小卷积核的卷积层代替一个卷积核较大的卷积层，一方面可以减少参数，另一方面相当于进行了更多的非线性映射，增强了模型的拟合表达能力\n",
    "#### 3. 小池化核\n",
    "#### 4. 通道数较多\n",
    "#### 5. 层数更深，特征图更宽\n",
    "#### 6. 全连接转卷积（测试阶段）\n",
    "##### 使得测试得到的卷积网络因为没有全连接的限制，因而可以接受任意宽或高的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例5 1】  Pytorch查看Vgg19网络结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg19()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 绕道下一层的工作机制：\n",
    "#### 1. 输入绕道（Skip Connection）：在残差块中，输入不仅直接传递给下一层（即第一个卷积层），而且通过一个绕道跳过这两个连续的卷积层，直接传递到这两个卷积层的输出端。这个绕道通常是一个恒等映射（identity mapping），意味着输入不经修改地通过绕道传递。\n",
    "\n",
    "#### 2. 输出合并：绕道的输出（即直接传递的输入）与两个卷积层的输出在通道维度上相加。这样，残差块的最终输出是这两个卷积层学习到的残差与原始输入的和。\n",
    "#### 引入残差，恒等映射（Identity Mapping），相当于一个梯度高速通道，可以更容易地训练，避免了梯度消失问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet的核心模块是残差模块，这里学习使用PyTorch搭建残差模块。\n",
    "#### 在实际项目中，模块当中通常封装为类，这里遵循这一传统，将残差模块封装为类，方便调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例5 2】  Pytorch残差构建模块封装成类。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock(\n",
      "  (conv1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        y = self.conv2(y)\n",
    "        return F.relu(x + y)\n",
    "\n",
    "net = ResidualBlock(4)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例5 3】  Pytorch实现嵌入残差模块的网络模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (rblock1): ResidualBlock(\n",
      "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (rblock2): ResidualBlock(\n",
      "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        y = self.conv2(y)\n",
    "        return F.relu(x + y)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=5)\n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        self.rblock1 = ResidualBlock(16)\n",
    "        self.rblock2 = ResidualBlock(32)\n",
    "\n",
    "        self.fc = torch.nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten data from (n,1,28,28) to (n,784)\n",
    "        in_size = x.size(0)\n",
    "        x = self.mp(F.relu(self.conv1(x)))\n",
    "        x = self.rblock1(x)\n",
    "        x = self.mp(F.relu(self.conv2(x)))\n",
    "        x = self.rblock2(x)\n",
    "        x = x.view(in_size, -1)  # flatten\n",
    "        #         print(x.size(1))\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例5 4】  Pytorch实现ResNet。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对应18层，34层的残差结构\n",
    "'''\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # 判断每一个卷积块中，卷积核的个数会不会有变化\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):  # downsample表示是否有升维操作\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # output = (input - kernel_size + 2*padding)/stride + 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1,\n",
    "                               bias=False)  # stride=1表示option A；stride=2表示optionB 使用BN不需要偏置bias\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "'''\n",
    "50层，101层，152层\n",
    "'''\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。\n",
    "    但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2，\n",
    "    这么做的好处是能够在top1上提升大概0.5%的准确率。\n",
    "    可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel * self.expansion,\n",
    "                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,  # 残差结构\n",
    "                 blocks_num,\n",
    "                 num_classes=1000,\n",
    "                 include_top=True,\n",
    "                 groups=1,\n",
    "                 width_per_group=64):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.in_channel = 64\n",
    "\n",
    "        self.groups = groups\n",
    "        self.width_per_group = width_per_group\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])  # 对应结构图中conv2_x，下面同理\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n",
    "        if self.include_top:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    '''\n",
    "    block: BasicBlock或Bottleneck\n",
    "    channel: 残差结构中的卷积核个数\n",
    "    block_num：这一层有多少残差结构，例：34的第一层有三个，第二层有四个\n",
    "    '''\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        # 快捷连接虚线部分\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        # 搭建每一个conv的第一层\n",
    "        layers.append(block(self.in_channel,\n",
    "                            channel,\n",
    "                            downsample=downsample,\n",
    "                            stride=stride,\n",
    "                            groups=self.groups,\n",
    "                            width_per_group=self.width_per_group))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel,\n",
    "                                channel,\n",
    "                                groups=self.groups,\n",
    "                                width_per_group=self.width_per_group))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.include_top:\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet34(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet50(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet101(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\n",
    "    groups = 32\n",
    "    width_per_group = 4\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3],\n",
    "                  num_classes=num_classes,\n",
    "                  include_top=include_top,\n",
    "                  groups=groups,\n",
    "                  width_per_group=width_per_group)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\n",
    "    groups = 32\n",
    "    width_per_group = 8\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3],\n",
    "                  num_classes=num_classes,\n",
    "                  include_top=include_top,\n",
    "                  groups=groups,\n",
    "                  width_per_group=width_per_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常规卷积是直接通过一个卷积核把空间信息和通道信息提取出来\n",
    "#### 空间分离卷积（spatial separable convolutions），顾名思义就是在空间维度将标准卷积运算进行拆分，将标准卷积核拆分成多个小卷积核。例如我们可以将卷积核拆分成两个（或多个）向量的外积：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$ \\begin{split}\\left[\\begin{array}{ccc}3 & 6 & 9 \\\\4 & 8 & 12 \\\\5 & 10 & 15\\end{array}\\right]=\t\\left[\\begin{array}{ccc}3 \\\\4 \\\\5\\end{array}\\right]\t\\times\\left[\\begin{array}{ccc}1 \\quad 2 \\quad 3\\end{array}\\right]\\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例5 5】  Pytorch实现深度可分离卷积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度可分离卷积\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "\n",
    "        # 逐通道卷积：groups=in_channels=out_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels,\n",
    "                               bias=bias)\n",
    "        # 逐点卷积：普通1x1卷积\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, groups=1,\n",
    "                                   bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例5 6】  Pytorch实现XceptionNet。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(Xception, self).__init__()\n",
    "        self.num_classes = num_classes  # 总分类数\n",
    "\n",
    "        ################################## 定义 Entry flow ###############################################################\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # do relu here\n",
    "\n",
    "        # Block中的参数顺序：in_filters,out_filters,reps,stride,start_with_relu,grow_first\n",
    "        self.block1 = Block(64, 128, 2, 2, start_with_relu=False, grow_first=True)\n",
    "        self.block2 = Block(128, 256, 2, 2, start_with_relu=True, grow_first=True)\n",
    "        self.block3 = Block(256, 728, 2, 2, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        ################################### 定义 Middle flow ############################################################\n",
    "        self.block4 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block5 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block6 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block7 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        self.block8 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block9 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block10 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block11 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        #################################### 定义 Exit flow ###############################################################\n",
    "        self.block12 = Block(728, 1024, 2, 2, start_with_relu=True, grow_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        # do relu here\n",
    "        self.conv4 = SeparableConv2d(1536, 2048, 3, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        ###################################################################################################################\n",
    "\n",
    "        # ------- init weights --------\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        # -----------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        ################################## 定义 Entry flow ###############################################################\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        ################################### 定义 Middle flow ############################################################\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "\n",
    "        #################################### 定义 Exit flow ###############################################################\n",
    "        x = self.block12(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【例4 14】  Pytorch实现XceNet的block。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, reps, strides=1, start_with_relu=True, grow_first=True):\n",
    "        #:parm reps:块重复次数\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        # Middle flow无需做这一步，而其余块需要，以做跳连\n",
    "        # 1）Middle flow输入输出特征图个数始终一致，且stride恒为1\n",
    "        # 2）其余块需要stride=2，这样可以将特征图尺寸减半，获得与最大池化减半特征图尺寸同样的效果\n",
    "        if out_filters != in_filters or strides != 1:\n",
    "            self.skip = nn.Conv2d(in_filters, out_filters, kernel_size=1, stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep = []\n",
    "\n",
    "        filters = in_filters\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            # 这里的卷积不改变特征图尺寸\n",
    "            rep.append(SeparableConv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps - 1):\n",
    "            rep.append(self.relu)\n",
    "            # 这里的卷积不改变特征图尺寸\n",
    "            rep.append(SeparableConv2d(filters, filters, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "\n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            # 这里的卷积不改变特征图尺寸\n",
    "            rep.append(SeparableConv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        # Middle flow 的stride恒为1，因此无需做池化，而其余块需要\n",
    "        # 其余块的stride=2，因此这里的最大池化可以将特征图尺寸减半\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(kernel_size=3, stride=strides, padding=1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x += skip\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
